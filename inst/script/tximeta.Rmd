---
title: "tximeta: Import transcript quantification with automagic generation of metadata"
author: "Michael Love, Rob Patro"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  html_document:
    highlight: tango
abstract: >
  `tximeta` performs numerous annotation and metadata gathering tasks on
  behalf of users during the import of transcript quantifications from
  Salmon or Sailfish into R/Bioconductor. The goal is to provide
  something similar to the experience of `GEOquery`, which downloaded
  microarray expression data from NCBI GEO and simultaneously brought
  along associated pieces of metadata. Doing this automatically helps to
  prevent costly bioinformatic errors. 
---

# This package is in beta 

[See README](https://github.com/mikelove/tximeta/blob/master/README.md)

# Setup

First, to try out `tximeta` you'll need the example data, which is
contained in this GitHub repo. Here we download the whole repo as a
ZIP file.

```{r echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

```{r eval=FALSE}
library(here)
dest <- here("extdata","asthma.zip")
download.file("https://github.com/mikelove/asthma/archive/master.zip", dest, method="wget")
unzip(dest, exdir=here("extdata"))
```

# Analysis starts with sample table

The first step using `tximeta` is to read in the sample table, which
will become the *column data*, `colData`, of the
*SummarizedExperiment*. This table should contain all the information
we need to identify the quant directories. In this case, we will use
the `run` ID from SRA.

```{r}
library(here)
library(readr)
coldata <- read_tsv(here("extdata","coldata.tsv"))
coldata
```

`tximeta` expects at least two columns in `coldata`: 

1. `files` - a pointer to the `quant.sf` files
2. `names` - the unique names that should be used to identify samples

We use `coldata$run` to build these two columns:

```{r}
coldata$files <- here("extdata","asthma-master","data","quants",
                      coldata$run,"quant.sf.gz")
coldata$names <- coldata$run
all(file.exists(coldata$files))
head(coldata$names)
```

# Running tximeta from a sample table

(Note: first do a `devtools::load_all()` then the following should work.)

```{r}
library(tximeta)
se <- tximeta(coldata)
```

# What happened? 

`tximeta` recognized the signature of the transcriptome
that the files were quantified against, it accessed the remote GTF
file of the transcriptome source, found and attached the transcript
genomic ranges, and added the appropriate transcriptome and genome metadata.
The remote GTF is only accessed once. If `tximeta` recognizes that it
has seen this index before, it will simply use a cached version of the
transcript metadata (this uses *BiocFileCache* and the specifics of
the cache location may change as `tximeta` develops).

We plan to create and maintain a large table of signatures for as many
sources, organisms, versions of transcriptomes as possible. We are
also developing support for "derived transcriptomes", where one or
more sources for transcript sequences have been merged or
filtered. See the `derivedTxome` vignette in this package for a
demonstration. 

# Examining SummarizedExperiment output

We, of course, have our coldata from before. Note that we've removed `files`.

```{r}
colData(se)
```

Here we show the three matrices that were imported. (Note: this part
would need updating for un-reduced inferential variance matrices.) 
(Second note: the downstream packages would need updating of their
functions or workflows, e.g. `DESeqDataSetFromTximport` needs a little
update to work with these *SummarizedExperiments* instead of simple lists.)

```{r}
assayNames(se)
```

Thanks to `tximeta` we have automagically imported the correct ranges
for the transcripts. 

```{r}
rowRanges(se)
```

We have appropriate genome information, which prevents us from making 
bioinformatic mistakes:

```{r}
seqinfo(se)
```

# Demo: find nearest transcripts to a ChIP-seq peak

Suppose we want to find overlap of the expression with binding sites
of a transcription factor:

```{r}
library(AnnotationHub)
ah <- AnnotationHub()
chip <- query(ah, c("GM12878", "MEF2A", "narrowPeak"))[[1]]
```

First try, let's find the nearest transcript to a given ChIP-seq peak:

```{r error=TRUE}
nearest(chip[1], se)
```

We get an <font color="red"><b>ERROR</b></font>: all chromosomes have
incompatibile genomes! Good! 
The point of `tximeta` is to reduce these kind of simple bioinformatic
mistakes that can add weeks or months of dead-end results to large
genomics projects. 
We can use liftover chains to get us going in the right direction,
comparing hg38 to hg38.

```{r eval=FALSE}
url <- "http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz"
file <- "hg19ToHg38.over.chain.gz"
if (!file.exists(file)) download.file(url, file, method="wget")
system(paste0("gunzip ",file))
```

We move our ChIP-seq data to hg38:

```{r}
chip.lift <- liftOverHelper(chip, chainfile="hg19ToHg38.over.chain", to="hg38")
```

Now we can find the nearest transcript to a given ChIP-seq peak:

```{r}
nearest(chip.lift[1], se)
assay(se)[nearest(chip.lift[1], se),,drop=FALSE]
```

Or we can take a slice of the transcriptome data that is within 1
megabase of a given ChIP-seq peak:

```{r}
# which rows of SE in this window?
which(overlapsAny(se, chip.lift[1] + 1e6))
```

Perhaps even more exciting, we can now automate functional annotation of
transcriptome data using Bioconductor's annotation suite.

# Metadata galore

```{r}
names(metadata(se))
str(metadata(se)$quantInfo)
str(metadata(se)$txomeInfo)
str(metadata(se)$tximetaInfo)
str(metadata(se)$txdbInfo)
```

# Next steps

### Basic functionality

* Switching `rowRanges` from transcript ranges to exons-by-transcript ranges list
* Summarization to gene-level (where building `tx2gene` is no longer necessary)
* As is already supported in the `tximport` release, also import inferential
  variance matrices (Gibbs samples or bootstrap samples)

### Facilitate plots and summaries
    
* Basic plots across samples: abundances, mapping rates, rich bias model parameters
* Time summaries: when quantified? when imported? I would love to
  know when the library was prepared and sequenced but this seems hopeless.

### Challenges

* Building out actual, sustainable plan for supporting as many
  organisms and sources as possible. We can define rules which
  determine where the FASTA and GTF files will be based on `source` and
  `version` (also here I ignored something like "type", e.g. CHR
  or ALL gene files from Gencode)
* Some support already for derived transcriptomes, see `derivedTxomes`
  vignette. Need to work more on combining multiple sources
  (potentially meta-transcriptomes from different organisms?), and
  also on how to approach de novo transcriptomes, and how to support
  reproducibility there.
* Facilitate functional annotation, either with vignettes/workflow or
  with additional functionality. E.g.: 
  housekeeping genes, arbitrary gene sets, genes expressed in GTEx tissues
* liftOver is clunky and doesn't integrate with
  GenomeInfoDb. It requires user input and there's a chance to
  mis-annotate. Ideally this should all be automated.

# Session info

```{r}
library(devtools)
session_info()
```
