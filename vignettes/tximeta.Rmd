---
title: "tximeta: Import transcript quantification with automagic generation of metadata"
author: "Michael Love, Rob Patro"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  html_document:
    highlight: tango
abstract: >
  `tximeta` performs numerous annotation and metadata gathering tasks on
  behalf of users during the import of transcript quantifications from
  Salmon or Sailfish into R/Bioconductor. The goal is to provide
  something similar to the experience of `GEOquery`, which downloaded
  microarray expression data from NCBI GEO and simultaneously brought
  along associated pieces of metadata. Doing this automatically helps to
  prevent costly bioinformatic errors. 
---

# This package is in beta 

[See README](https://github.com/mikelove/tximeta/blob/master/README.md)

```{r echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

# Analysis starts with sample table

The first step using `tximeta` is to read in the sample table, which
will become the *column data*, `colData`, of the
*SummarizedExperiment*. This table should contain all the information
we need to identify the quant directories. Here we will use the
*Salmon* quantification files in the *tximportData* package to
demonstrate the usage of `tximeta`. We do not have a sample table, so
we construct one in R, but it is recommended to keep a sample table as
a CSV or TSV file while working on a project.

```{r}
dir <- system.file("extdata/salmon", package="tximportData")
ids <- list.files(dir)
# here gzipped, normally these are not
files <- file.path(dir, ids, "quant.sf.gz") 
file.exists(files)
coldata <- data.frame(files, names=ids, population="TSI", stringsAsFactors=FALSE)
coldata
```

`tximeta` expects at least two columns in `coldata`: 

1. `files` - a pointer to the `quant.sf` files
2. `names` - the unique names that should be used to identify samples

# Running tximeta from a sample table

(Note: first do a `devtools::load_all()` then the following should work.)

```{r}
library(tximeta)
se <- tximeta(coldata)
```

# What happened? 

`tximeta` recognized the signature of the transcriptome
that the files were quantified against, it accessed the remote GTF
file of the transcriptome source, found and attached the transcript
genomic ranges, and added the appropriate transcriptome and genome metadata.
The remote GTF is only accessed once. If `tximeta` recognizes that it
has seen this index before, it will simply use a cached version of the
transcript metadata (this uses *BiocFileCache* and the specifics of
the cache location may change as `tximeta` develops).

We plan to create and maintain a large table of signatures for as many
sources, organisms, versions of transcriptomes as possible. We are
also developing support for "linked transcriptomes", where one or
more sources for transcript sequences have been merged or
filtered. See the `linkedTxome` vignette in this package for a
demonstration. 

# Examining SummarizedExperiment output

We, of course, have our coldata from before. Note that we've removed `files`.

```{r}
colData(se)
```

Here we show the three matrices that were imported. (Note: this part
would need updating for un-reduced inferential variance matrices.) 
(Second note: the downstream packages would need updating of their
functions or workflows, e.g. `DESeqDataSetFromTximport` needs a little
update to work with these *SummarizedExperiments* instead of simple lists.)

```{r}
assayNames(se)
```

Thanks to `tximeta` we have automagically imported the correct ranges
for the transcripts. 

```{r}
rowRanges(se)
```

We have appropriate genome information, which prevents us from making 
bioinformatic mistakes:

```{r}
seqinfo(se)
```

# Easy summarization to gene-level

Because the SummarizedExperiment maintains all the metadata of its
creation, it also keeps a pointer to the necessary database for
summarizing transcript-level quantifications and bias corrections to
the gene-level. If necessary, `summarizeToGene` can pull down the
remote source for summarization, but given that we've already built a
TxDb once, it simply loads the stashed version.

```{r}
gse <- summarizeToGene(se)
rowRanges(gse)
```

# Add different identifiers

We would like to add support to easily map transcript or gene
identifiers from one annotation to another. This is just a prototype
function, but we show how we can easily add alternate IDs given that we
know the organism and the source of the transcriptome. (This function
currently only works for Gencode and Ensembl gene or transcript IDs
for human, but could be extended to work for arbitrary sources.)

```{r}
library(Homo.sapiens)
gse <- addIds(gse, "SYMBOL")
mcols(gse)[1:10,]
```

# Run a differential expression analysis

The following code chunk demonstrates how to build a *DESeqDataSet*
and begin a differential expression analysis. Likely we would
simplify these steps with a convenience function either in *tximeta*
or in *DESeq2*.

```{r}
suppressPackageStartupMessages(library(DESeq2))
gse2 <- gse
assayNames(gse2)
# rounding counts
assay(gse2) <- round(assay(gse2))
# rename the "length" assay to "avgTxLength"
# DESeq2 will then use this matrix for calculating bias offsets
assayNames(gse2)[3] <- "avgTxLength"
# make a DESeqDataSet, here there is no factor
# to divide the samples so we use ~1
dds <- DESeqDataSet(gse2, ~1)
dds <- estimateSizeFactors(dds)
# ... and so on
```

# Find nearest transcripts to a ChIP-seq peak

Suppose we want to find overlap of the expression with binding sites
of a transcription factor:

```{r}
library(AnnotationHub)
ah <- AnnotationHub()
chip <- query(ah, c("GM12878", "MEF2A", "narrowPeak"))[[1]]
```

First try, let's find the nearest transcript to a given ChIP-seq peak:

```{r error=TRUE}
nearest(chip[1], se)
```

We get an <font color="red"><b>ERROR</b></font>: all chromosomes have
incompatibile genomes! Good! 
The point of `tximeta` is to reduce these kind of simple bioinformatic
mistakes that can add weeks or months of dead-end results to large
genomics projects. 
We can use liftover chains to get us going in the right direction,
comparing hg38 to hg38 (this code chunk un-evaluated).

```{r eval=FALSE}
url <- "http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz"
file <- "hg19ToHg38.over.chain.gz"
if (!file.exists(file)) download.file(url, file, method="wget")
system(paste("gunzip ",file))
```

```{r}
chainfile <- system.file("extdata/hg19ToHg38.over.chain", package="tximeta")
```

We move our ChIP-seq data to hg38:

```{r}
chip.lift <- liftOverHelper(chip, chainfile=chainfile, to="hg38")
```

Now we can find the nearest transcript to a given ChIP-seq peak:

```{r}
nearest(chip.lift[1], se)
assay(se)[nearest(chip.lift[1], se),,drop=FALSE]
```

Or we can take a slice of the transcriptome data that is within 10 kb
of a given ChIP-seq peak:

```{r}
# which rows of SE in this window?
which(overlapsAny(se, chip.lift[1] + 1e4))
```

Perhaps even more exciting, we can now automate functional annotation of
transcriptome data using Bioconductor's annotation suite.

# Metadata galore

```{r}
names(metadata(se))
str(metadata(se)$quantInfo)
str(metadata(se)$txomeInfo)
str(metadata(se)$tximetaInfo)
str(metadata(se)$txdbInfo)
```

# Next steps

### Basic functionality

* Switching `rowRanges` from transcript ranges to exons-by-transcript
  ranges list, or from gene ranges to exons-by-gene ranges list.
* As is already supported in the `tximport` release, also import inferential
  variance matrices (Gibbs samples or bootstrap samples)

### Facilitate plots and summaries
    
* Basic plots across samples: abundances, mapping rates, rich bias model parameters
* Time summaries: when quantified? when imported? I would love to
  know when the library was prepared and sequenced but this seems hopeless.

### Challenges

* Building out actual, sustainable plan for supporting as many
  organisms and sources as possible. We can define rules which
  determine where the FASTA and GTF files will be based on `source` and
  `version` (also here I ignored something like "type", e.g. CHR
  or ALL gene files from Gencode)
* Some support already for linked transcriptomes, see `linkedTxomes`
  vignette. Need to work more on combining multiple sources
  (potentially meta-transcriptomes from different organisms?), and
  also on how to approach de novo transcriptomes, and how to support
  reproducibility there.
* Facilitate functional annotation, either with vignettes/workflow or
  with additional functionality. E.g.: 
  housekeeping genes, arbitrary gene sets, genes expressed in GTEx tissues
* `liftOver` is clunky and doesn't integrate with
  *GenomeInfoDb*. It requires user input and there's a chance to
  mis-annotate. Ideally this should all be automated.

# Session info

```{r}
library(devtools)
session_info()
```
